{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Softmax-Logistic-Regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sthalles/logistic-regression/blob/master/Softmax_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "YFQ4kO9C4Ouw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from numpy.linalg import pinv, inv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DpZ5as2oVN2-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DataSet:\n",
        "  def __init__(self, data, targets, valid_classes=None):\n",
        "    if valid_classes is None:\n",
        "      self.valid_classes = np.unique(targets)\n",
        "    else:\n",
        "      self.valid_classes = valid_classes\n",
        "    #print(self.valid_classes)\n",
        "    self.number_of_classes = len(self.valid_classes)\n",
        "    self.data = self.to_dict(data, targets)\n",
        "    \n",
        "    total = 0\n",
        "    for i in self.data.keys():\n",
        "      print(\"Class {0} # of records: {1}\".format(i,len(self.data[i])))\n",
        "      total += len(self.data[i])\n",
        "    print(\"Total:\",total)\n",
        "    \n",
        "  def to_dict(self, data, targets):\n",
        "    data_dict = {}\n",
        "    for x, y in zip(data, targets):\n",
        "      if y in self.valid_classes:\n",
        "        if y not in data_dict:\n",
        "          data_dict[y] = [x.flatten()]\n",
        "        else:\n",
        "          data_dict[y].append(x.flatten())\n",
        "\n",
        "    for i in self.valid_classes:\n",
        "      data_dict[i] = np.asarray(data_dict[i])\n",
        "\n",
        "    return data_dict\n",
        "\n",
        "  def get_data_by_class(self, class_id):\n",
        "    if class_id in self.valid_classes:\n",
        "      return self.data[class_id]\n",
        "    else:\n",
        "      raise (\"Class not found.\")\n",
        "\n",
        "  def get_data_as_dict(self):\n",
        "    return self.data\n",
        "\n",
        "  def get_all_data(self):\n",
        "    data = []\n",
        "    labels = []\n",
        "    for label, class_i_data in self.data.items():\n",
        "      data.extend(class_i_data)\n",
        "      labels.extend(class_i_data.shape[0] * [label])\n",
        "    data = np.asarray(data)\n",
        "    labels = np.asarray(labels)\n",
        "    return data, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jt-2WI-QMY0V",
        "colab_type": "code",
        "outputId": "c7aa6681-f30f-4160-e43f-b51cb01af8fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pvkCe0BjVQrf",
        "colab_type": "code",
        "outputId": "931eb819-f671-4492-97d5-057f87edc711",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "train_dataset = DataSet(x_train, y_train, valid_classes=[0,1,2,3,4])\n",
        "x_train, y_train = train_dataset.get_all_data()\n",
        "\n",
        "test_dataset = DataSet(x_test, y_test, valid_classes=[0,1,2,3,4])\n",
        "x_test, y_test = test_dataset.get_all_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class 0 # of records: 5923\n",
            "Class 4 # of records: 5842\n",
            "Class 1 # of records: 6742\n",
            "Class 2 # of records: 5958\n",
            "Class 3 # of records: 6131\n",
            "Total: 30596\n",
            "Class 2 # of records: 1032\n",
            "Class 1 # of records: 1135\n",
            "Class 0 # of records: 980\n",
            "Class 4 # of records: 982\n",
            "Class 3 # of records: 1010\n",
            "Total: 5139\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "h_vimCHieaeq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ones = np.ones((x_train.shape[0],1))\n",
        "# x_train = np.concatenate((ones,x_train),axis=1)\n",
        "\n",
        "# ones = np.ones((x_test.shape[0],1))\n",
        "# x_test = np.concatenate((ones,x_test),axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ofSyeC79Ng8B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train = np.reshape(x_train, (x_train.shape[0],-1))\n",
        "x_test = np.reshape(x_test, (x_test.shape[0],-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YMOsG28rM5a0",
        "colab_type": "code",
        "outputId": "8a98eb90-9052-4740-8676-aa803309cc23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# BATCH_SIZE=18623\n",
        "\n",
        "# x_train = x_train[:BATCH_SIZE,:]\n",
        "# y_train = y_train[:BATCH_SIZE]\n",
        "\n",
        "k=len(np.unique(y_train))\n",
        "print(\"# of classes:\",k)\n",
        "print(\"Input shape:\",x_train.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of classes: 5\n",
            "Input shape: (30596, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k3drQYrOTA4c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def to_one_hot(targets,k):\n",
        "  onehot = np.zeros((targets.shape[0], k))\n",
        "  for i,t in enumerate(targets):\n",
        "    onehot[i][t] = 1\n",
        "  return onehot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lxs9tvKkMsUs",
        "colab_type": "code",
        "outputId": "3ca06dca-ecab-441d-cade-127dcf7d8e20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# y_train = to_one_hot(y_train, k)\n",
        "# y_test = to_one_hot(y_test, k)\n",
        "\n",
        "print(\"targets shape:\",y_train.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "targets shape: (30596,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8Y2F7dXYtB__",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# targets = np.argmax(y_test,1)\n",
        "# print(targets.shape)\n",
        "# print(targets)\n",
        "# np.sum(targets == np.zeros([4157])) / len(targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_gULSNVI4P8s",
        "colab_type": "code",
        "outputId": "f7732bbe-47f7-4ac6-835a-1733f7769569",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "W = np.full((x_train.shape[1],k), 0.01)\n",
        "#W = np.random.randn(x_train.shape[1],k)*np.sqrt(2/k) #np.random.rand(x_train.shape[1],k)\n",
        "print(W)\n",
        "print(\"weights:\",W.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.01 0.01 0.01 0.01 0.01]\n",
            " [0.01 0.01 0.01 0.01 0.01]\n",
            " [0.01 0.01 0.01 0.01 0.01]\n",
            " ...\n",
            " [0.01 0.01 0.01 0.01 0.01]\n",
            " [0.01 0.01 0.01 0.01 0.01]\n",
            " [0.01 0.01 0.01 0.01 0.01]]\n",
            "weights: (784, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rEKLFbmX4igp",
        "colab_type": "code",
        "outputId": "008567ec-690b-436a-81b2-ff2621488428",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "linear = np.dot(x_train,W)\n",
        "print(\"Linear combination:\",linear.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear combination: (30596, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0TfEAgOa4rF4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def softmax(logits):\n",
        "  # Input x result of linear combination np.dot(x.W), shape = (BATCH_SIZE,k)\n",
        "  activations = np.zeros((logits.shape))\n",
        "  \n",
        "  for k in range(logits.shape[1]):\n",
        "    safe_logits = logits - np.max(logits)\n",
        "    denominator = np.sum(np.exp(safe_logits),axis=1)\n",
        "    # print(\"denominator shape:\",denominator.shape)\n",
        "    activations[:,k] = np.divide(np.exp(safe_logits[:,k]), denominator)\n",
        "  return activations"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xASNtvIB5K82",
        "colab_type": "code",
        "outputId": "bea557f7-b1b3-4207-8de9-32702ec614d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "activations = softmax(np.dot(x_train,W))\n",
        "print(\"activations:\",activations.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "activations: (30596, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pcqLecWz5ffm",
        "colab_type": "code",
        "outputId": "b1a6a14f-a5d8-4647-bef6-11a3948dcdb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# validate softmax function\n",
        "idx=10\n",
        "print(activations[idx], np.sum(activations[idx]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.2 0.2 0.2 0.2 0.2] 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OY6dp9jT6dpk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# gradients = np.dot(x_train.T, np.subtract(y_train, activations))\n",
        "# print(\"gradients\",gradients.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nEPlSlCYqmDS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _softmax(a):\n",
        "    a_max = np.max(a, axis=-1, keepdims=True)\n",
        "    exp_a = np.exp(a - a_max)\n",
        "    return exp_a / np.sum(exp_a, axis=-1, keepdims=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CY3RQ9cryp4t",
        "colab_type": "code",
        "outputId": "b6a08d67-eaa4-4c66-d922-2c1a78f6ebc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "X = x_train\n",
        "T = y_train\n",
        "\n",
        "X_ = x_test\n",
        "T_ = y_test\n",
        "\n",
        "print(X.shape,T.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30596, 784) (30596,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KjQxaJEUHgtO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def grad(y,t,x):\n",
        "  # y: models predictions (Batch,)\n",
        "  # t: dataset targets (Batch,)\n",
        "  return np.dot(x.T, (y - t))\n",
        "  \n",
        "  \n",
        "class LogisticRegression:\n",
        "  def __init__(self,fit_intercept=True):\n",
        "    self.fit_intercept = fit_intercept\n",
        "    self.W = None\n",
        "  \n",
        "  def score(self,X,y):\n",
        "    \"\"\"\n",
        "    X: Logits from softmax(W^Tx). shape: [N,K]\n",
        "    y: true targets. shape [N]\n",
        "    \"\"\"\n",
        "    if self.fit_intercept:\n",
        "      X = self.add_intercept(X)    \n",
        "      \n",
        "    logits = self.forward(X)\n",
        "    pred = np.argmax(logits,axis=1)\n",
        "    return np.sum(pred == y) / len(y)\n",
        "    \n",
        "  def _softmax(self, a):\n",
        "    \"\"\"\n",
        "    a: Linear combination of inputs and weights: shape: [N,K] \n",
        "    \"\"\"\n",
        "    return np.exp(a) / np.sum(np.exp(a), axis=-1, keepdims=True)\n",
        "    \n",
        "  def forward(self,X):\n",
        "    \"\"\"\n",
        "    Model the posterior probability P(Ck|x) as a softmax function on the linear combination of inputs X and weights W.\n",
        "    X: inputs, shape: [N,K]\n",
        "    return: softmax transformation probabilities.\n",
        "    \"\"\"\n",
        "    logits = np.dot(X,self.W)\n",
        "    \n",
        "    safe_logits = logits - np.max(logits, axis=-1, keepdims=True)\n",
        "    return self._softmax(logits) # the order dot(x,W) seems correct\n",
        "\n",
        "  def add_intercept(self,x):\n",
        "    # generate a NxM design matrix, with an added column of 1\n",
        "    const = np.ones((x.shape[0],1))\n",
        "    return np.concatenate((const,x),axis=1)\n",
        "\n",
        "  def fit(self,X,y,iterations=2):\n",
        "    \"\"\"\n",
        "    Fit K-1 lines to the input data X.\n",
        "    X: inputs, shape: [N,K]\n",
        "    y: true targets. shape [N]\n",
        "    iterations: max number of iterations for training\n",
        "    \"\"\"\n",
        "    if self.fit_intercept:\n",
        "      X = self.add_intercept(X)\n",
        "    \n",
        "    self.k = len(np.unique(y))\n",
        "    self.d = X.shape[1]\n",
        "    dk = self.d*self.k\n",
        "    \n",
        "    print(\"Input shape:\",X.shape)\n",
        "    print(\"# classes: {0} features dim:{1}\".format(self.k,self.d))\n",
        "    \n",
        "    # convert the labels to one-hot encoding\n",
        "    y = to_one_hot(y, self.k)\n",
        "    \n",
        "    HT = np.zeros((self.d,self.k,self.d,self.k))\n",
        "      \n",
        "    if self.W is None:\n",
        "      self.W = np.zeros([self.d, self.k])\n",
        "      W_shape = self.W.shape\n",
        "    \n",
        "    for i in range(iterations):\n",
        "      logits = self.forward(X)\n",
        "      for i in range(k):\n",
        "        for j in range(k):\n",
        "          r = np.multiply(logits[:,i],((i==j)-logits[:,j]))  ## r has negative value, so cannot use sqrt\n",
        "          HT[:,i,:,j] = np.dot(np.multiply(X.T,r),X) # 4.110      \n",
        "      \n",
        "      G = np.dot(X.T,(logits-y))\n",
        "      H = np.reshape(HT,(dk,dk))\n",
        "      self.W = self.W.reshape(-1) - np.dot(pinv(H), G.reshape(-1)) # 4.92\n",
        "      # W = W - 0.001 * G\n",
        "      # print(np.min(W),np.max(W))\n",
        "      self.W = np.reshape(self.W,W_shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tGMvHn8QIw4r",
        "colab_type": "code",
        "outputId": "c0a04ecd-b839-42ca-d2e2-5007110aa6fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "clf = LogisticRegression(fit_intercept=True)\n",
        "clf.fit(x_train,y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input shape: (30596, 785)\n",
            "# classes: 5 features dim:785\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "28mKzU4IKB85",
        "colab_type": "code",
        "outputId": "4a81f78a-9664-4c43-df8a-09a00dcb38d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.score(x_test,y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9673088149445418"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "83vJ183ZZbcF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}