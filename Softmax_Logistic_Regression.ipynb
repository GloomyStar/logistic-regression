{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Softmax-Logistic-Regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sthalles/logistic-regression/blob/master/Softmax_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "YFQ4kO9C4Ouw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from numpy.linalg import pinv, inv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DpZ5as2oVN2-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DataSet:\n",
        "  def __init__(self, data, targets, valid_classes=None):\n",
        "    if valid_classes is None:\n",
        "      self.valid_classes = np.unique(targets)\n",
        "    else:\n",
        "      self.valid_classes = valid_classes\n",
        "    #print(self.valid_classes)\n",
        "    self.number_of_classes = len(self.valid_classes)\n",
        "    self.data = self.to_dict(data, targets)\n",
        "    \n",
        "    total = 0\n",
        "    for i in self.data.keys():\n",
        "      print(\"Class {0} # of records: {1}\".format(i,len(self.data[i])))\n",
        "      total += len(self.data[i])\n",
        "    print(\"Total:\",total)\n",
        "    \n",
        "  def to_dict(self, data, targets):\n",
        "    data_dict = {}\n",
        "    for x, y in zip(data, targets):\n",
        "      if y in self.valid_classes:\n",
        "        if y not in data_dict:\n",
        "          data_dict[y] = [x.flatten()]\n",
        "        else:\n",
        "          data_dict[y].append(x.flatten())\n",
        "\n",
        "    for i in self.valid_classes:\n",
        "      data_dict[i] = np.asarray(data_dict[i])\n",
        "\n",
        "    return data_dict\n",
        "\n",
        "  def get_data_by_class(self, class_id):\n",
        "    if class_id in self.valid_classes:\n",
        "      return self.data[class_id]\n",
        "    else:\n",
        "      raise (\"Class not found.\")\n",
        "\n",
        "  def get_data_as_dict(self):\n",
        "    return self.data\n",
        "\n",
        "  def get_all_data(self):\n",
        "    data = []\n",
        "    labels = []\n",
        "    for label, class_i_data in self.data.items():\n",
        "      data.extend(class_i_data)\n",
        "      labels.extend(class_i_data.shape[0] * [label])\n",
        "    data = np.asarray(data)\n",
        "    labels = np.asarray(labels)\n",
        "    return data, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jt-2WI-QMY0V",
        "colab_type": "code",
        "outputId": "49377166-0693-44aa-d7df-676d05509eb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pvkCe0BjVQrf",
        "colab_type": "code",
        "outputId": "255f79d0-437e-469e-c046-5dcac149e015",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "cell_type": "code",
      "source": [
        "train_dataset = DataSet(x_train, y_train, valid_classes=[0,1,2,3,4])\n",
        "x_train, y_train = train_dataset.get_all_data()\n",
        "\n",
        "test_dataset = DataSet(x_test, y_test, valid_classes=[0,1,2,3,4])\n",
        "x_test, y_test = test_dataset.get_all_data()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class 0 # of records: 5923\n",
            "Class 4 # of records: 5842\n",
            "Class 1 # of records: 6742\n",
            "Class 2 # of records: 5958\n",
            "Class 3 # of records: 6131\n",
            "Total: 30596\n",
            "Class 2 # of records: 1032\n",
            "Class 1 # of records: 1135\n",
            "Class 0 # of records: 980\n",
            "Class 4 # of records: 982\n",
            "Class 3 # of records: 1010\n",
            "Total: 5139\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ofSyeC79Ng8B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train = np.reshape(x_train, (x_train.shape[0],-1))\n",
        "x_test = np.reshape(x_test, (x_test.shape[0],-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k3drQYrOTA4c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def to_one_hot(targets,k):\n",
        "  onehot = np.zeros((targets.shape[0], k))\n",
        "  for i,t in enumerate(targets):\n",
        "    onehot[i][t] = 1\n",
        "  return onehot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KjQxaJEUHgtO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LogisticRegression:\n",
        "  def __init__(self,fit_intercept=True):\n",
        "    self.fit_intercept = fit_intercept\n",
        "    self.W = None\n",
        "  \n",
        "  def score(self,X,y):\n",
        "    \"\"\"\n",
        "    X: Logits from softmax(W^Tx). shape: [N,K]\n",
        "    y: true targets. shape [N]\n",
        "    \"\"\"\n",
        "    if self.fit_intercept:\n",
        "      X = self.add_intercept(X)    \n",
        "      \n",
        "    logits = self.forward(X)\n",
        "    pred = np.argmax(logits,axis=1)\n",
        "    return np.sum(pred == y) / len(y)\n",
        "    \n",
        "  def predict(self,X):\n",
        "    \"\"\"\n",
        "    X: Logits from softmax(W^Tx). shape: [N,K]\n",
        "    \"\"\"\n",
        "    if self.fit_intercept:\n",
        "      X = self.add_intercept(X)    \n",
        "      \n",
        "    logits = self.forward(X)\n",
        "    pred = np.argmax(logits,axis=1)\n",
        "    return pred\n",
        "    \n",
        "  def _softmax(self, a):\n",
        "    \"\"\"\n",
        "    a: Linear combination of inputs and weights: shape: [N,K] \n",
        "    \"\"\"\n",
        "    return np.exp(a) / np.sum(np.exp(a), axis=-1, keepdims=True)\n",
        "    \n",
        "  def forward(self,X):\n",
        "    \"\"\"\n",
        "    Model the posterior probability P(Ck|x) as a softmax function on the linear combination of inputs X and weights W.\n",
        "    X: inputs, shape: [N,K]\n",
        "    return: softmax transformation probabilities.\n",
        "    \"\"\"\n",
        "    logits = np.dot(X,self.W)\n",
        "    \n",
        "    safe_logits = logits - np.max(logits, axis=-1, keepdims=True)\n",
        "    return self._softmax(logits) # the order dot(x,W) seems correct\n",
        "\n",
        "  def add_intercept(self,x):\n",
        "    # generate a NxM design matrix, with an added column of 1\n",
        "    const = np.ones((x.shape[0],1))\n",
        "    return np.concatenate((const,x),axis=1)\n",
        "\n",
        "  def fit(self,X,y,iterations=2):\n",
        "    \"\"\"\n",
        "    Fit K-1 lines to the input data X.\n",
        "    X: inputs, shape: [N,K]\n",
        "    y: true targets. shape [N]\n",
        "    iterations: max number of iterations for training\n",
        "    \"\"\"\n",
        "    if self.fit_intercept:\n",
        "      X = self.add_intercept(X)\n",
        "      \n",
        "    # get the number of classes\n",
        "    k = len(np.unique(y))\n",
        "    \n",
        "    # get the data dimensionality\n",
        "    d = X.shape[1] \n",
        "    \n",
        "    # compute the dimension of the Hessian\n",
        "    dk = d*k\n",
        "    \n",
        "    print(\"Input shape:\",X.shape)\n",
        "    print(\"# classes: {0} features dim:{1}\".format(k,d))\n",
        "    \n",
        "    # convert the labels to one-hot encoding\n",
        "    y = to_one_hot(y, k)\n",
        "    \n",
        "    HT = np.zeros((d,k,d,k))\n",
        "      \n",
        "    if self.W is None:\n",
        "      self.W = np.zeros([d, k])\n",
        "      W_shape = self.W.shape\n",
        "    \n",
        "    for i in range(iterations):\n",
        "      logits = self.forward(X)\n",
        "      for i in range(k):\n",
        "        for j in range(k):\n",
        "          r = np.multiply(logits[:,i],((i==j)-logits[:,j]))  ## r has negative value, so cannot use sqrt\n",
        "          HT[:,i,:,j] = np.dot(np.multiply(X.T,r),X) # 4.110      \n",
        "      \n",
        "      G = np.dot(X.T,(logits-y))\n",
        "      H = np.reshape(HT,(dk,dk))\n",
        "      print(\"Hessian:\",H.shape)\n",
        "      self.W = self.W.reshape(-1) - np.dot(pinv(H), G.reshape(-1)) # 4.92\n",
        "      # W = W - 0.001 * G\n",
        "      # print(np.min(W),np.max(W))\n",
        "      self.W = np.reshape(self.W,W_shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tGMvHn8QIw4r",
        "colab_type": "code",
        "outputId": "26675a7f-a498-45c9-895f-11d231ded750",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "clf = LogisticRegression(fit_intercept=True)\n",
        "clf.fit(x_train,y_train)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input shape: (30596, 785)\n",
            "# classes: 5 features dim:785\n",
            "Hessian: (3925, 3925)\n",
            "Hessian: (3925, 3925)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "28mKzU4IKB85",
        "colab_type": "code",
        "outputId": "1b1d9ce8-743b-4f2a-bf34-68dd7396364e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Test accuracy:\",clf.score(x_test,y_test))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.9673088149445418\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "83vJ183ZZbcF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}